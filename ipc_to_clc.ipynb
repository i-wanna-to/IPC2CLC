{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipc和clc实验数据自动提取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重复的ipc分类号： A41B17/00\n",
      "重复的ipc分类号： A01C3/04\n",
      "重复的ipc分类号： A61K9/20\n",
      "重复的ipc分类号： A01K77/00\n",
      "重复的ipc分类号： A01B3/04\n",
      "重复的ipc分类号： B60L7/22\n",
      "重复的ipc分类号： B28C5/42\n",
      "重复的ipc分类号： B64D29/00\n",
      "重复的ipc分类号： C22C30/02\n",
      "重复的ipc分类号： E21F13/04\n",
      "重复的ipc分类号： E01F7/00\n",
      "重复的ipc分类号： F21W106/00\n",
      "重复的ipc分类号： F21W106/00\n",
      "重复的ipc分类号： F02K3/00\n",
      "重复的ipc分类号： F42C19/08\n",
      "重复的ipc分类号： G06C15/02\n",
      "重复的ipc分类号： H02K25/00\n",
      "ipc_dict:  356\n",
      "ipc_key_list:  373\n",
      "ipc_value_list:  373\n",
      "dup ipc_key_list:  356\n",
      "dup ipc_value_list:  360\n"
     ]
    }
   ],
   "source": [
    "#读入ipc文件后value中没有空格了（有重复的数据代表不是一一映射，并不是错误）\n",
    "ipc_dict = {}\n",
    "ipc_key_list = []\n",
    "ipc_value_list = []\n",
    "with open(\"./ipc/ipc_keywords_sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        \n",
    "        assert len(line) == 2, '错误出现在key：' + str(line[0]) + '!'\n",
    "        if str(line[0]) in ipc_key_list:\n",
    "            print('重复的ipc分类号：', str(line[0]))\n",
    "            \n",
    "        ipc_key_list.append(str(line[0]))\n",
    "        ipc_str = ''\n",
    "        for index in range(1, len(line)):\n",
    "            ipc_str = ipc_str + str(line[index])\n",
    "        ipc_dict[str(line[0])] = ipc_str\n",
    "        ipc_value_list.append(ipc_str)\n",
    "\n",
    "print('ipc_dict: ', len(ipc_dict))\n",
    "print('ipc_key_list: ', len(ipc_key_list))\n",
    "print('ipc_value_list: ', len(ipc_value_list))\n",
    "print('dup ipc_key_list: ', len(set(ipc_key_list)))\n",
    "print('dup ipc_value_list: ', len(set(ipc_value_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clc有重复的需要在后面一个clc号上加上  '+1' , ipc重复的ipc分类号不用管"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clc_dict:  373\n",
      "clc_key_list:  373\n",
      "clc_value_list:  373\n",
      "dup clc_key_list:  373\n",
      "dup clc_value_list:  359\n"
     ]
    }
   ],
   "source": [
    "#读入clc文件后value中没有空格了（有重复的数据代表不是一一映射，并不是错误）\n",
    "clc_dict = {}\n",
    "clc_key_list = []\n",
    "clc_value_list = []\n",
    "\n",
    "with open(\"./clc/clc_keywords_sentences_dup.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        \n",
    "        assert len(line) == 2, '错误出现在key：' + str(line[0]) + '!'\n",
    "        if str(line[0]) in clc_key_list:\n",
    "            print('重复的clc分类号：', str(line[0]))\n",
    "            \n",
    "        clc_key_list.append(str(line[0]))\n",
    "        clc_str = ''\n",
    "        for index in range(1, len(line)):\n",
    "            clc_str = clc_str + str(line[index])\n",
    "        clc_dict[str(line[0])] = clc_str\n",
    "        clc_value_list.append(clc_str)\n",
    "\n",
    "print('clc_dict: ', len(clc_dict))\n",
    "print('clc_key_list: ', len(clc_key_list))\n",
    "print('clc_value_list: ', len(clc_value_list))\n",
    "print('dup clc_key_list: ', len(set(clc_key_list)))\n",
    "print('dup clc_value_list: ', len(set(clc_value_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#统计哪些clc的key重复了\n",
    "from collections import Counter\n",
    "\n",
    "clc_key_list_dict = dict(Counter(clc_key_list))\n",
    "\n",
    "print({key:value for key,value in clc_key_list_dict.items() if value > 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#对重复clc的key的进行修改，使其不重复\\nclc_key_dup_dict = {}\\nfor clc_key1 in clc_key_list:\\n    clc_key_dup_dict[clc_key1] = 0\\n\\nf_clc_write = open('./clc/clc_keywords_dup.txt', 'w', encoding='utf-8')\\n\\nfor clc_key2, clc_value2 in zip(clc_key_list, clc_value_list):\\n    clc_key_dup_dict[clc_key2] += 1\\n    if clc_key_dup_dict[clc_key2] == 1:\\n        f_clc_write.write(clc_key2 + ' ' + clc_value2 + '\\n')\\n    else:\\n        f_clc_write.write(clc_key2 + '+' + str(clc_key_dup_dict[clc_key2]-1) + ' ' + clc_value2 + '\\n')\\n\\nf_clc_write.close()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#对重复clc的key的进行修改，使其不重复\n",
    "clc_key_dup_dict = {}\n",
    "for clc_key1 in clc_key_list:\n",
    "    clc_key_dup_dict[clc_key1] = 0\n",
    "\n",
    "f_clc_write = open('./clc/clc_keywords_dup.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for clc_key2, clc_value2 in zip(clc_key_list, clc_value_list):\n",
    "    clc_key_dup_dict[clc_key2] += 1\n",
    "    if clc_key_dup_dict[clc_key2] == 1:\n",
    "        f_clc_write.write(clc_key2 + ' ' + clc_value2 + '\\n')\n",
    "    else:\n",
    "        f_clc_write.write(clc_key2 + '+' + str(clc_key_dup_dict[clc_key2]-1) + ' ' + clc_value2 + '\\n')\n",
    "\n",
    "f_clc_write.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.597 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#jieba用来分词和词性标注以及提取关键词\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#给结巴添加自定义词典\n",
    "jieba.load_userdict(\"./word_embedding/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#要过滤的词性\n",
    "filter_characters = ['t', 'TIME', 'w', 'm', 'xc', 'nz', 'c', 'p']\n",
    "#要过滤的中英文符号\n",
    "ignored_characters = ['()', '{}', '{', '}', '[]', '*', '=', '+', '<>', '<', '>', '&', '#', \\\n",
    "                    '@', '~', '%', '$', '|', '（）', '（', '）', '『』', '『', '』', '【】', \\\n",
    "                    '【', '】', '‘', '￥', '～', '……', '——', '“”', '“', '”', '：', '《', '》', \\\n",
    "                    '《》', '？', '、']\n",
    "#提取关键字时允许的词性\n",
    "#allow_pos_characters = ('n', 'f', 's', 'nr', 'ns', 'nt', 'nw', 'v', 'vd', 'vn', \\\n",
    "#                        'a', 'ad', 'an', 'd', 'r', 'p', 'u', 'PER', 'LOC', 'ORG')\n",
    "allow_pos_characters = ('n', 'f', 's', 'nr', 'ns', 'nt', 'nw', 'v', 'vd', 'vn', \\\n",
    "                        'a', 'ad', 'an', 'd', 'r', 'p', 'u', 'PER', 'LOC', 'ORG',\n",
    "                       'nz', 'l', 'b', 'nrt', 'z', 'g', 'vg', 'nrfg', 'nrf', 'nis', \\\n",
    "                        'j', 'nhd', 'ng', 'i', 'nnd', 'nit', 'nnt', 'gi', 'gm', 'gp', \\\n",
    "                        'gm', 'gg', 'gb', 'zg', 'nba', 'nnt', 'nf', 'gc', 'nmc', 'mq')\n",
    "\n",
    "def is_in_allow_pos_characters(flag):\n",
    "    return True if flag in allow_pos_characters else False\n",
    "\n",
    "def not_in_filter_characters(flag):\n",
    "    return True if flag not in filter_characters else False\n",
    "\n",
    "def not_in_ignored_characters(word):\n",
    "    return True if word not in ignored_characters else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对clc提取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_drop_characters_dict = dict()\n",
    "clc_key_words_list = []\n",
    "\n",
    "\n",
    "for key, value in zip(clc_key_list, clc_value_list):\n",
    "    #当value中只有一个字的时候，则不进行提取关键词\n",
    "    if len(value) > 1:\n",
    "        #使用TF-IDF算法和TextRank算法提取关键词，并取两者的并集，可以指定提取关键词的个数(topK)和提取关键词的词性(allowPOS)\n",
    "        list1 = jieba.analyse.extract_tags(value, topK=30, withWeight=False, \\\n",
    "                                        allowPOS=allow_pos_characters)\n",
    "        list2 = jieba.analyse.textrank(value, topK=30, withWeight=False, \\\n",
    "                                    allowPOS=allow_pos_characters)\n",
    "\n",
    "        #取并集并去重\n",
    "        dedup_clc_key_words_list = list(set(list1).union(set(list2)))\n",
    "        #提取关键词要大于等于１\n",
    "        if len(dedup_clc_key_words_list) >= 1:\n",
    "            clc_key_words_str = ''\n",
    "            for value0 in dedup_clc_key_words_list:\n",
    "                clc_key_words_str = clc_key_words_str + str(value0) + ';'\n",
    "\n",
    "            clc_key_words_list.append(str(clc_key_words_str))\n",
    "        \n",
    "        #提取不到关键词,则进行分词，由词性标注提取关键词\n",
    "        else:\n",
    "            words = pseg.cut(value) #jieba默认模式\n",
    "            save_drop_characters_dict[key] = ''\n",
    "            clc_key_words_str = ''\n",
    "            for word, flag in words:\n",
    "                if is_in_allow_pos_characters(flag):\n",
    "                    clc_key_words_str = clc_key_words_str + str(word) + ';'\n",
    "                else:\n",
    "                    save_drop_characters_dict[key] = save_drop_characters_dict[key] + str(word) + str(flag) + ';'\n",
    "            #分词后也要大于1，否则不分词\n",
    "            if len(clc_key_words_str) >= 1:\n",
    "                clc_key_words_list.append(str(clc_key_words_str))\n",
    "            else:\n",
    "                clc_key_words_list.append(str(value))\n",
    "                            \n",
    "    #不提取关键词\n",
    "    elif len(value) == 1:\n",
    "        clc_key_words_list.append(str(value))\n",
    "    else:\n",
    "        print('error: ', key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_drop_characters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clc_key_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w = open('./clc/clc_keywords_sentences_dup_by_jieba.txt', 'w', encoding='utf-8')\n",
    "assert len(clc_key_list) == len(clc_key_words_list), '提取关键词有误!'\n",
    "for key, value in zip(clc_key_list, clc_key_words_list):\n",
    "    f_w.write(key + ' ' + value + '\\n')\n",
    "f_w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对ipc提取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_drop_characters_dict = dict()\n",
    "ipc_key_words_list = []\n",
    "\n",
    "\n",
    "for key, value in zip(ipc_key_list, ipc_value_list):\n",
    "    #当value中只有一个字的时候，则不进行提取关键词\n",
    "    if len(value) > 1:\n",
    "        #使用TF-IDF算法和TextRank算法提取关键词，并取两者的并集，可以指定提取关键词的个数(topK)和提取关键词的词性(allowPOS)\n",
    "        list1 = jieba.analyse.extract_tags(value, topK=30, withWeight=False, \\\n",
    "                                        allowPOS=allow_pos_characters)\n",
    "        list2 = jieba.analyse.textrank(value, topK=30, withWeight=False, \\\n",
    "                                    allowPOS=allow_pos_characters)\n",
    "\n",
    "        #取并集并去重\n",
    "        dedup_ipc_key_words_list = list(set(list1).union(set(list2)))\n",
    "        #提取关键词要大于等于１\n",
    "        if len(dedup_ipc_key_words_list) >= 1:\n",
    "            ipc_key_words_str = ''\n",
    "            for value0 in dedup_ipc_key_words_list:\n",
    "                ipc_key_words_str = ipc_key_words_str + str(value0) + ';'\n",
    "\n",
    "            ipc_key_words_list.append(str(ipc_key_words_str))\n",
    "        \n",
    "        #提取不到关键词,则进行分词，由词性标注提取关键词\n",
    "        else:\n",
    "            words = pseg.cut(value) #jieba默认模式\n",
    "            save_drop_characters_dict[key] = ''\n",
    "            ipc_key_words_str = ''\n",
    "            for word, flag in words:\n",
    "                if is_in_allow_pos_characters(flag):\n",
    "                    ipc_key_words_str = ipc_key_words_str + str(word) + ';'\n",
    "                else:\n",
    "                    save_drop_characters_dict[key] = save_drop_characters_dict[key] + str(word) + str(flag) + ';'\n",
    "            #分词后也要大于1，否则不分词\n",
    "            if len(ipc_key_words_str) >= 1:\n",
    "                ipc_key_words_list.append(str(ipc_key_words_str))\n",
    "            else:\n",
    "                ipc_key_words_list.append(str(value))\n",
    "                            \n",
    "    #不提取关键词\n",
    "    elif len(value) == 1:\n",
    "        ipc_key_words_list.append(str(value))\n",
    "    else:\n",
    "        print('error: ', key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_drop_characters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ipc_key_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w = open('./ipc/ipc_keywords_sentences_by_jieba.txt', 'w', encoding='utf-8')\n",
    "assert len(ipc_key_list) == len(ipc_key_words_list), '提取关键词有误!'\n",
    "for key, value in zip(ipc_key_list, ipc_key_words_list):\n",
    "    f_w.write(key + ' ' + value + '\\n')\n",
    "f_w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动构造输入文件input.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造ipc输入文件\n",
    "import pandas as pd\n",
    "import xlwt\n",
    "import re\n",
    "\n",
    "pattern = '[A-H]{1}[0-9]{2}[A-Z]{1}[0-9]{1,4}\\/[0-9]{2,6}' #匹配ipc分类号的正则表达式\n",
    "f_ipc_r = open('./ipc/ipc_keywords.txt', 'r', encoding='utf-8') #关键词\n",
    "f_clc_r = open('./clc/clc_keywords.txt', 'r', encoding='utf-8') #关键词\n",
    "\n",
    "clc_list = []\n",
    "for line in f_clc_r:\n",
    "    line = line.strip()\n",
    "    line_list = line.split(' ')\n",
    "    clc_list.append(str(line_list[0]))\n",
    "    \n",
    "\n",
    "writebook = xlwt.Workbook()  #打开一个excel\n",
    "sheet = writebook.add_sheet('ipc_input')  #在打开的excel中添加一个sheet\n",
    "\n",
    "sheet.write(0, 0, 'IPC分类号') #写入excel，1行0列\n",
    "sheet.write(0, 1, 'IPC描述') #写入excel，1行1列\n",
    "sheet.write(0, 2, 'CLC分类号') #写入excel，1行2列\n",
    "\n",
    "#控制sheet的行数\n",
    "i = 0\n",
    "for line in f_ipc_r:\n",
    "    i = i + 1\n",
    "    line = line.strip()\n",
    "    line_list = line.split(' ')\n",
    "    #匹配不上返回 None\n",
    "    match_str = re.search(pattern, line_list[0])\n",
    "    if match_str is None:\n",
    "        print('匹配不上：', line_list)\n",
    "    else:\n",
    "        if len(line_list) > 2:\n",
    "            print('>2 error : ', line_list)\n",
    "        elif len(line_list) == 2:\n",
    "            sheet.write(i, 0, line_list[0]) #写入excel，i行0列\n",
    "            sheet.write(i, 1, line_list[1]) #写入excel，i行1列\n",
    "            sheet.write(i, 2, clc_list[i-1]) #写入excel，i行2列\n",
    "        else:\n",
    "            print('<2 error : ', line_list)\n",
    "\n",
    "writebook.save('./data/input_manmaked.xlsx')#一定要记得保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于类目相似度的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.585 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_open library not found; falling back to local-filesystem-only\n",
      "[jieba] default dict file path ../data/vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/hw/anaconda3/envs/transquest/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\n",
      "Loading model from cache /tmp/jieba.u0aee24de5e38baa78ebdb92867f204df.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms load wordseg dict [/home/hw/anaconda3/envs/transquest/lib/python3.8/site-packages/synonyms/data/vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [/home/hw/anaconda3/envs/transquest/lib/python3.8/site-packages/synonyms/data/stopwords.txt] ...\n",
      "[Synonyms] on loading vectors [/home/hw/anaconda3/envs/transquest/lib/python3.8/site-packages/synonyms/data/words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import warnings\n",
    "import ipdb\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#jieba用来分词和词性标注以及提取关键词\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#给结巴添加自定义词典\n",
    "jieba.load_userdict(\"./word_embedding/vocab.txt\")\n",
    "\n",
    "#用来使用word2vec词向量进行匹配（近义词匹配）\n",
    "import synonyms\n",
    "\n",
    "#fuzzywuzzy库用来匹配字符串的相似度\n",
    "#固定一个词不动；另一个词的长短对匹配无影响\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入的参数/地址\n",
    "input_dir = './data/input_manmaked.xlsx'\n",
    "output_dir = './data/output_manmaked_373.xlsx'\n",
    "nbest = 10  #就是top_k\n",
    "auto_get_keywords = False  #与input_dir相关，如果input_dir输入的是已经分词的，则为False，否则为True\n",
    "using_TF_IDF = False  #与auto_get_keywords相关，当auto_get_keywords为True时生效。表示是否使用TF_IDF提取关键词。\n",
    "\n",
    "ipc_dir = './ipc/ipc_2019.txt'  #这个可以不用了\n",
    "ipc_sentence_dir = './ipc/ipc_keywords_sentences.txt'  #只有句子级别的匹配才用得上(因为一对多，所以ipc会存在重复，这个没关系)\n",
    "\n",
    "clc_dir = './clc/clc_keywords_sentences_dup.txt'  #clc的句子(重复的分类号需要处理一下)\n",
    "clc_key_words_dir = './clc/clc_keywords_dup.txt'  #clc的关键词(重复的分类号需要处理一下)\n",
    "\n",
    "\n",
    "#synonyms近义词匹配的个数，这是一个很重要的超参数，初步验证近义词个数太大效果变差（5->10 : 52%->45%）\n",
    "#需要多尝试，默认为 10\n",
    "nearby_number = 5\n",
    "\n",
    "\n",
    "#四选一\n",
    "only_using_fuzzy = False\n",
    "only_using_synonyms_keywords, nearby_number = False, nearby_number\n",
    "only_using_synonyms_sentence, no_symbol = False, False  #传入匹配的时候需不需要标点符号 ;\n",
    "using_fuzzy_and_synonyms, nearby_number = True, nearby_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造ipc字典 （现在已经可以不用了）\n",
    "ipc_dict = {}\n",
    "ipc_key_list = []\n",
    "\n",
    "with open(ipc_dir, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        ipc_key_list.append(str(line[0]))\n",
    "        ipc_dict[str(line[0])] = []\n",
    "        for index in range(1, len(line)):\n",
    "            ipc_dict[str(line[0])].append(str(line[index]))\n",
    "            \n",
    "#ipc_dict: {'A':['XXX'], 'B':['YYYY'], ...}\n",
    "\n",
    "\n",
    "#构造ipc　sentence字典 （只有句子级别匹配需要用到）\n",
    "ipc_sentence_dict = {}\n",
    "ipc_sentence_key_list = []\n",
    "ipc_sentence_value_list = []\n",
    "\n",
    "with open(ipc_sentence_dir, 'r', encoding='utf-8') as f:\n",
    "    key_i = 0\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        ipc_sentence_key_list.append(str(line[0]))\n",
    "        ipc_sentence_value_list.append(str(line[1]))\n",
    "        ipc_sentence_dict[str(key_i)] = []  #因为ipc的分类号有重复，所以不用ipc分类号作为key\n",
    "        assert len(line) == 2, 'clc句子中存在空格！'\n",
    "        \n",
    "        ipc_sentence_dict[str(key_i)].append(str(line[1]))\n",
    "        key_i = key_i + 1\n",
    "        \n",
    "#ipc_sentence_dict: {'0':['XXX'], '1':['YYYY'], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造clc字典  clc的句子\n",
    "clc_dict = dict()\n",
    "clc_key_list = []\n",
    "clc_value_list = []\n",
    "\n",
    "with open(clc_dir, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        clc_key_list.append(str(line[0]))\n",
    "        clc_value_list.append(str(line[1]))\n",
    "        clc_dict[str(line[0])] = []\n",
    "        assert len(line) == 2, 'clc句子中存在空格！'\n",
    "        \n",
    "        for index in range(1, len(line)):\n",
    "            clc_dict[str(line[0])].append(str(line[index]))\n",
    "\n",
    "#clc_dict: {'A':['XXX'], 'B':['YYYY'], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用来统计一个ipc包含多个clc的clc字典\n",
    "match_clc_key_dict = dict()\n",
    "\n",
    "for ipc_key_i, target in enumerate(ipc_sentence_key_list):\n",
    "    match_clc_key_dict[str(ipc_key_i)] = []\n",
    "    for ipc_key_j, every_ipc_key in enumerate(ipc_sentence_key_list):\n",
    "        if every_ipc_key == target:\n",
    "            #key是数字序号，和clc分类号顺序是一致的 ，有1000条数据\n",
    "            match_clc_key_dict[str(ipc_key_i)].append(clc_key_list[ipc_key_j])\n",
    "\n",
    "#用于计算IPC每一类别包含的数量\n",
    "ipc_section_number_dict = {\n",
    "    'A' : 0,\n",
    "    'B' : 0,\n",
    "    'C' : 0,\n",
    "    'D' : 0,\n",
    "    'E' : 0,\n",
    "    'F' : 0,\n",
    "    'G' : 0,\n",
    "    'H' : 0\n",
    "}\n",
    "for ipc_key_i in ipc_sentence_key_list:\n",
    "    ipc_section_number_dict[str(ipc_key_i[0])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个clc字典，用来存储匹配得分，因为clc对重复的数据进行了+1处理，所以是1000条数据\n",
    "rank_clc_dict = dict()\n",
    "for key, value in clc_dict.items():\n",
    "    rank_clc_dict[str(key)] = 0\n",
    "    \n",
    "#clc_dict: {'A':0, 'B':0, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n"
     ]
    }
   ],
   "source": [
    "#构造clc key words字典, 因为clc对重复的数据进行了+1处理，所以是1000条数据\n",
    "clc_key_words_dict = dict()\n",
    "clc_key_words_key_list = []\n",
    "save_null_list_dict = {}\n",
    "\n",
    "with open(clc_key_words_dir, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        clc_key_words_key_list.append(str(line[0]))\n",
    "        clc_key_words_dict[str(line[0])] = []\n",
    "        save_null_list_dict[str(line[0])] = []\n",
    "        assert len(line) == 2\n",
    "        \n",
    "        key_words_value_list = line[1].split(';')\n",
    "        for index in range(len(key_words_value_list)):\n",
    "            if str(key_words_value_list[index]) != '':\n",
    "                clc_key_words_dict[str(line[0])].append(str(key_words_value_list[index]))\n",
    "            else:\n",
    "                save_null_list_dict[str(line[0])].append(str(key_words_value_list[index]))\n",
    "                \n",
    "print(len(save_null_list_dict))\n",
    "assert len(clc_dict) == len(clc_key_words_dict), 'clc句子和关键词的字典大小需要一致！'\n",
    "\n",
    "#clc_key_words_dict: {'A':['XXX', 'WW'], 'B':['YYYY', 'QQ', 'E'], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用来统计字典的值有没有超过1的\n",
    "for k,v in clc_dict.items():\n",
    "    if len(v) > 1:\n",
    "        print(k, v)\n",
    "        \n",
    "for k,v in ipc_dict.items():\n",
    "    if len(v) > 1:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 'F21W131/4035')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用来统计最长的ipc和clc的分类号\n",
    "max_len = 0\n",
    "max_len_value = ''\n",
    "for io in ipc_key_list:\n",
    "    if len(io) > max_len:\n",
    "        max_len = len(io)\n",
    "        max_len_value = io\n",
    "max_len, max_len_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前时间【2021-02-25 23:14:34】已经执行到第 1 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:15:14】已经执行到第 51 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:15:45】已经执行到第 101 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:16:13】已经执行到第 151 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:16:31】已经执行到第 201 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:16:50】已经执行到第 251 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:17:13】已经执行到第 301 个ipc分类号了！\n",
      "\n",
      "当前时间【2021-02-25 23:17:36】已经执行到第 351 个ipc分类号了！\n",
      "\n",
      "\n",
      "Final acc: 64.08% .\n",
      "\n",
      "Final top_10_acc: 94.91% .\n",
      "\n",
      "总数：373\n",
      "top1 正确个数：239\n",
      "top1 错误个数：134\n",
      "\n",
      "总数：373\n",
      "topk 正确个数：354\n",
      "topk 错误个数：19\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "Final top_2_acc: 78.82% .\n",
      "\n",
      "Final top_3_acc: 84.99% .\n",
      "\n",
      "Final top_4_acc: 87.94% .\n",
      "\n",
      "Final top_5_acc: 90.08% .\n",
      "\n",
      "Final top_6_acc: 91.15% .\n",
      "\n",
      "Final top_7_acc: 92.23% .\n",
      "\n",
      "Final top_8_acc: 93.57% .\n",
      "\n",
      "Final top_9_acc: 94.64% .\n",
      "\n",
      "Final top_10_acc: 94.91% .\n",
      "\n",
      "--------------------\n",
      "\n",
      "总数：373\n",
      "top2 正确个数：294\n",
      "top2 错误个数：79\n",
      "\n",
      "总数：373\n",
      "top3 正确个数：317\n",
      "top3 错误个数：56\n",
      "\n",
      "总数：373\n",
      "top4 正确个数：328\n",
      "top4 错误个数：45\n",
      "\n",
      "总数：373\n",
      "top5 正确个数：336\n",
      "top5 错误个数：37\n",
      "\n",
      "总数：373\n",
      "top6 正确个数：340\n",
      "top6 错误个数：33\n",
      "\n",
      "总数：373\n",
      "top7 正确个数：344\n",
      "top7 错误个数：29\n",
      "\n",
      "总数：373\n",
      "top8 正确个数：349\n",
      "top8 错误个数：24\n",
      "\n",
      "总数：373\n",
      "top9 正确个数：353\n",
      "top9 错误个数：20\n",
      "\n",
      "总数：373\n",
      "top10 正确个数：354\n",
      "top10 错误个数：19\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "类别：A，总数为：85，正确个数为：47，错误个数为：38，正确率为：55.29%\n",
      "\n",
      "类别：B，总数为：71，正确个数为：50，错误个数为：21，正确率为：70.42%\n",
      "\n",
      "类别：C，总数为：49，正确个数为：31，错误个数为：18，正确率为：63.27%\n",
      "\n",
      "类别：D，总数为：24，正确个数为：10，错误个数为：14，正确率为：41.67%\n",
      "\n",
      "类别：E，总数为：24，正确个数为：20，错误个数为：4，正确率为：83.33%\n",
      "\n",
      "类别：F，总数为：40，正确个数为：26，错误个数为：14，正确率为：65.00%\n",
      "\n",
      "类别：G，总数为：40，正确个数为：35，错误个数为：5，正确率为：87.50%\n",
      "\n",
      "类别：H，总数为：40，正确个数为：20，错误个数为：20，正确率为：50.00%\n",
      "\n",
      "\n",
      "程序执行完毕！\n"
     ]
    }
   ],
   "source": [
    "#匹配ipc分类号的正则表达式\n",
    "pattern = '[A-H]{1}[0-9]{2}[A-Z]{1}[0-9]{1,4}\\/[0-9]{2,6}'\n",
    "#读入要匹配的输入文件，路径注意转义 \n",
    "df = pd.read_excel(input_dir, sheet_name=None)\n",
    "#i用来标识当前是第几个子表(sheet)\n",
    "i = 0\n",
    "#向同一个Excel中写入多个子表(sheet)\n",
    "writer = pd.ExcelWriter(output_dir)\n",
    "\n",
    "\n",
    "#df.keys()为每个子表sheet的名称，df.values()为每个子表sheet的内容\n",
    "for sheet in df.values():\n",
    "    #在子表中新增三列，并赋初值\n",
    "    col_name = sheet.columns.tolist()\n",
    "    col_name.insert(len(col_name)+1, 'CLC分类号汇总')\n",
    "    sheet = sheet.reindex(columns = col_name)\n",
    "    sheet['CLC分类号汇总'] = ['0' for f_z in sheet['IPC分类号']]\n",
    "    \n",
    "    col_name.insert(len(col_name)+2, 'CLC描述')\n",
    "    sheet = sheet.reindex(columns = col_name)\n",
    "    sheet['CLC描述'] = [' ' for f_z in sheet['CLC描述']]\n",
    "    \n",
    "    col_name.insert(len(col_name)+3, 'CLC匹配频次')\n",
    "    sheet = sheet.reindex(columns = col_name)\n",
    "    sheet['CLC匹配频次'] = ['0' for f_z in sheet['CLC匹配频次']]\n",
    "    \n",
    "    #读出 'IPC分类号'\n",
    "    sheet1 = sheet['IPC分类号']\n",
    "    input_ipc = []  #存储要匹配的ipc分类号\n",
    "    is_matched_flag = []  #标记ipc分类号在ipc字典中是否可以查到\n",
    "    \n",
    "    #存储匹配正确的数量(只取匹配度最高的)\n",
    "    correct = 0\n",
    "    \n",
    "    error_match_ipc_clc_pair = []\n",
    "\n",
    "    #存储TopK中匹配正确的数量\n",
    "    top_k_correct = 0\n",
    "    \n",
    "    #存储top2-TopK中匹配正确的数量(包含了topK)\n",
    "    top2_to_topk_correct_dict = {\n",
    "        '2' : 0,\n",
    "        '3' : 0,\n",
    "        '4' : 0,\n",
    "        '5' : 0,\n",
    "        '6' : 0,\n",
    "        '7' : 0,\n",
    "        '8' : 0,\n",
    "        '9' : 0,\n",
    "        '10' : 0\n",
    "    }\n",
    "    \n",
    "    #存储每一个IPC类别的正确个数\n",
    "    ipc_section_correct_dict = {\n",
    "        'A' : 0,\n",
    "        'B' : 0,\n",
    "        'C' : 0,\n",
    "        'D' : 0,\n",
    "        'E' : 0,\n",
    "        'F' : 0,\n",
    "        'G' : 0,\n",
    "        'H' : 0\n",
    "    }\n",
    "    \n",
    "    topk_error_match_ipc_clc_pair = []\n",
    "    \n",
    "    #遍历当前子表(sheet)的每一个ipc分类号\n",
    "    for j in range(len(sheet1)):\n",
    "        #取出IPC分类号，并转成字符串\n",
    "        sheet1[j] = str(sheet1[j])\n",
    "        line = sheet1[j].strip()\n",
    "        \n",
    "        #返回line中第一个匹配上的ipc号，没有则返回None\n",
    "        match_str = re.search(pattern, line)\n",
    "        \n",
    "        #匹配上了，并且存在ipc字典中（全部取excel表中的数据，if判断直接为False）\n",
    "        if False and (match_str is not None) and (str(match_str.group()) in ipc_key_list):\n",
    "            is_matched_flag.append(True)\n",
    "            input_ipc.append(str(match_str.group()))\n",
    "        else:\n",
    "            is_matched_flag.append(False)\n",
    "            #print('当前第{}个子表，第{}个IPC号：[{}]格式不正确或者ipc字典中没有这个ipc分类号！将自动提取sheet的“IPC描述”内容作为当前的匹配字符串！'.format(i+1, j+1, line))\n",
    "            #print()\n",
    "            input_ipc.append(str(sheet['IPC描述'][j]))\n",
    "            \n",
    "    '------------句子级别匹配---------------'\n",
    "    if only_using_synonyms_sentence:\n",
    "        input_ipc = []\n",
    "        for ipc_kk, ipc_vv_list in ipc_sentence_dict.items():\n",
    "            input_ipc.append(ipc_vv_list)\n",
    "    \n",
    "    '------------句子级别匹配---------------'\n",
    "    \n",
    "            \n",
    "    for ipc_i, current_input_ipc in enumerate(input_ipc):\n",
    "        #每个ipc分类号都要重新初始化clc字典，用来存储匹配得分\n",
    "        for key, value in rank_clc_dict.items():\n",
    "            rank_clc_dict[str(key)] = 0\n",
    "            \n",
    "        '''---------------这一部分已经不用了------------------'''\n",
    "        #匹配上了，则到ipc字典中取（is_matched_flag中全为False）\n",
    "        if is_matched_flag[ipc_i]:\n",
    "            current_ipc_description = []  #存储当前ipc的描述\n",
    "            \n",
    "            section_ipc_description = []  #存储当前ipc“部”的描述\n",
    "            main_class_ipc_description = []#存储当前ipc“大类”的描述\n",
    "            sub_class_ipc_description = []#存储当前ipc“小类”的描述\n",
    "            main_group_ipc_description = []#存储当前ipc“主组”的描述\n",
    "            #存储经过处理后的ipc描述（包含上述所有的描述）\n",
    "            ipc_description = []\n",
    "\n",
    "            #从ipc字典中获得当前输入的ipc描述，之后将当前ipc的所有父节点（上一级）的描述并入当前ipc的描述中\n",
    "            current_ipc_description = ipc_dict.get(current_input_ipc)\n",
    "\n",
    "            #1 取“部”的描述\n",
    "            section_ipc_description = ipc_dict.get(current_input_ipc[0])\n",
    "            ipc_description.extend(section_ipc_description)\n",
    "\n",
    "            #2 取“大类”的描述\n",
    "            main_class_ipc_description = ipc_dict.get(current_input_ipc[0:3])\n",
    "            ipc_description.extend(main_class_ipc_description)\n",
    "\n",
    "            #3 取“小类”的描述\n",
    "            sub_class_ipc_description = ipc_dict.get(current_input_ipc[0:4])\n",
    "            ipc_description.extend(sub_class_ipc_description)\n",
    "\n",
    "            #4 判断当前 current_input_ipc 是否为 00 结尾\n",
    "            #为 00 结尾：直接结束(得到分组的描述)\n",
    "            #不为 00 结尾：将 00 结尾的描述并入当前ipc的描述中\n",
    "            two_item_list = current_input_ipc.split('/')\n",
    "            if str(two_item_list[1]) == '00':\n",
    "                ipc_description.extend(current_ipc_description)\n",
    "            else:\n",
    "                #4 取“主组”的描述\n",
    "                main_group_ipc_description = ipc_dict.get(str(two_item_list[0]) + '/00')\n",
    "                ipc_description.extend(main_group_ipc_description)\n",
    "                ipc_description.extend(current_ipc_description)\n",
    "\n",
    "            #将ipc_description列表中的描述全部连接起来，作为一个ipc匹配（查询）字符串\n",
    "            ipc_query_string = ''\n",
    "            for each_ipc_description in ipc_description:\n",
    "                ipc_query_string = ipc_query_string + str(each_ipc_description)\n",
    "        \n",
    "            '''---------------这一部分已经不用了------------------'''\n",
    "        #否则没有匹配上。\n",
    "        else:\n",
    "            #如果没有匹配上，则直接取sheet表中的“IPC描述”作为当前ipc匹配（查询）字符串\n",
    "            ipc_query_string = current_input_ipc\n",
    "        \n",
    "        if auto_get_keywords:\n",
    "            if using_TF_IDF:\n",
    "                #使用TF-IDF算法和TextRank算法提取关键词，并取两者的并集，可以指定提取关键词的个数(topK)和提取关键词的词性(allowPOS)\n",
    "                list1 = jieba.analyse.extract_tags(ipc_query_string, topK=30, withWeight=False, \\\n",
    "                                                    allowPOS=allow_pos_characters)\n",
    "                list2 = jieba.analyse.textrank(ipc_query_string, topK=30, withWeight=False, \\\n",
    "                                                allowPOS=allow_pos_characters)\n",
    "\n",
    "                #取并集并去重\n",
    "                dedup_ipc_key_words_list = list(set(list1).union(set(list2)))\n",
    "            \n",
    "            #直接使用jieba分词作为关键字\n",
    "            else:\n",
    "                ipc_query_string_list = jieba.lcut(ipc_query_string, HMM=True)\n",
    "                ipc_query_string_list = list(set(ipc_query_string_list))\n",
    "                vv_list = []\n",
    "                for vv_str1 in ipc_query_string_list:\n",
    "                    if vv_str1 != ';' and vv_str1 != '（' and vv_str1 != '）' and vv_str1 != '，' \\\n",
    "                        and vv_str1 != '、' and vv_str1 != '，':\n",
    "                        vv_list.append(vv_str1)\n",
    "\n",
    "                dedup_ipc_key_words_list = vv_list\n",
    "\n",
    "        else:\n",
    "            if only_using_synonyms_sentence:\n",
    "                dedup_ipc_key_words_list = ipc_query_string\n",
    "            else:\n",
    "                dedup_ipc_key_words_list = list(set(ipc_query_string.rstrip(';').split(';')))\n",
    "        \n",
    "        ''' print('第', ipc_i, '个ipc关键词个数为：', len(dedup_ipc_key_words_list)) '''\n",
    "        ''' print('第', ipc_i, '个ipc关键词为：', dedup_ipc_key_words_list) '''\n",
    "        \n",
    "        if only_using_fuzzy:\n",
    "            #对ipc中每一个关键字在clc字典中作查询/匹配\n",
    "            for each_ipc_key_words in dedup_ipc_key_words_list:\n",
    "                for clc_key, clc_value in clc_dict.items():\n",
    "                    #字形匹配上了，该clc分类号+1\n",
    "                    if each_ipc_key_words in clc_value[0]:\n",
    "                        rank_clc_dict[clc_key] = rank_clc_dict[clc_key] + 1\n",
    "                    #匹配不上，继续\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                        \n",
    "        if only_using_synonyms_keywords:\n",
    "            #对ipc的句子与clc句子进行匹配，得分最高的即为相互映射\n",
    "            for each_ipc_key_words in dedup_ipc_key_words_list:\n",
    "                for clc_key, clc_value in clc_key_words_dict.items():\n",
    "                    for clc_value0 in clc_value:\n",
    "                        #获取ipc的近义词列表\n",
    "                        ipc_nearby_words_tuple = synonyms.nearby(each_ipc_key_words, nearby_number)\n",
    "                        ipc_nearby_words_list = ipc_nearby_words_tuple[0]  #元组第一个为近义词，第二个为近义词距离得分\n",
    "                        #当OOV时，返回[]，则忽略匹配\n",
    "                        if len(ipc_nearby_words_list) > 0:\n",
    "                            #近义词匹配上了，该clc分类号+1\n",
    "                            if clc_value0 in ipc_nearby_words_list:\n",
    "                                rank_clc_dict[clc_key] = rank_clc_dict[clc_key] + 1\n",
    "                                break\n",
    "                            #匹配不上，继续\n",
    "                            else:\n",
    "                                continue\n",
    " \n",
    "        \n",
    "        if only_using_synonyms_sentence:\n",
    "            #对ipc中每一个关键字在clc字典中作查询/匹配\n",
    "            for clc_key, clc_sentence_list in clc_dict.items():\n",
    "                if no_symbol:#去掉标点符号　;\n",
    "                    s1 = ''.join(''.join(dedup_ipc_key_words_list).rstrip(';').split(';'))\n",
    "                    s2 = ''.join(''.join(clc_sentence_list).rstrip(';').split(';'))\n",
    "                else:\n",
    "                    s1 = ''.join(dedup_ipc_key_words_list)\n",
    "                    s2 = ''.join(clc_sentence_list)\n",
    "                #利用synonyms进行word2vec句子语义匹配\n",
    "                s_r = synonyms.compare(s1, s2, seg=True, \\\n",
    "                                       ignore=True, \\\n",
    "                                       stopwords=False\n",
    "                                      )\n",
    "                rank_clc_dict[clc_key] = s_r\n",
    "                            \n",
    "            \n",
    "        #1月6日进行了修改，准确率有提升    \n",
    "        if using_fuzzy_and_synonyms:\n",
    "            #对ipc中每一个关键字在clc字典中作查询/匹配\n",
    "            for each_ipc_key_words in dedup_ipc_key_words_list:\n",
    "                for clc_key, clc_value in clc_dict.items():\n",
    "                    #字形匹配上了，该clc分类号+1\n",
    "                    if each_ipc_key_words in clc_value[0]:\n",
    "                        rank_clc_dict[clc_key] = rank_clc_dict[clc_key] + 1\n",
    "                    #匹配不上，继续用近义词匹配\n",
    "                    else:\n",
    "                        clc_value_description = clc_key_words_dict.get(clc_key)\n",
    "                        #获取ipc关键词的近义词\n",
    "                        ipc_nearby_words_tuple = synonyms.nearby(each_ipc_key_words, nearby_number)\n",
    "                        #近义词匹配上了，该clc分类号+1\n",
    "                        ipc_nearby_words_list = ipc_nearby_words_tuple[0]  #元组第一个为近义词，第二个为近义词距离得分\n",
    "                        \n",
    "                        #如果当前ipc关键词在synonyms中找不到关键词，即属于oov，synonyms返回 ([], [])；\n",
    "                        #则当前ipc关键词不进行匹配，这里出现oov不会报警告。\n",
    "                        if len(ipc_nearby_words_list) > 0:\n",
    "                            for ipc_value0 in ipc_nearby_words_list:\n",
    "                                if ipc_value0 in clc_value_description:\n",
    "                                    rank_clc_dict[clc_key] = rank_clc_dict[clc_key] + 1\n",
    "                                    break\n",
    "                                #匹配不上，继续\n",
    "                                else:\n",
    "                                    continue\n",
    "            \n",
    "            #还属于 using_fuzzy_and_synonyms  --方案一 （这个效果最差）\n",
    "            #for clc_k_1, clc_v_1 in clc_key_words_dict.items():\n",
    "            #    rank_clc_dict[clc_k_1] = rank_clc_dict[clc_k_1]/len(clc_v_1)\n",
    "                \n",
    "            #还属于 using_fuzzy_and_synonyms  --方案二 （这个结合方案三效果最好）\n",
    "            ipc_to_match_sentence = ipc_sentence_dict[str(ipc_i)][0]\n",
    "            for clc_k_1, clc_v_1 in clc_dict.items():\n",
    "                rank_clc_dict[clc_k_1] = rank_clc_dict[clc_k_1] + (fuzz.ratio(ipc_to_match_sentence, clc_v_1[0])/100)\n",
    "        \n",
    "        #方案三（仅使用这个效果也还不错，加上句子级别的匹配效果更差）\n",
    "        #ipc_to_match_sentence = ipc_sentence_dict[str(ipc_i)][0]\n",
    "        #for clc_k_1, clc_v_1 in clc_dict.items():\n",
    "        #    rank_clc_dict[clc_k_1] = fuzz.ratio(ipc_to_match_sentence, clc_v_1[0])\n",
    "            #rank_clc_dict[clc_k_1] = (fuzz.ratio(ipc_to_match_sentence, clc_v_1[0])/100) + synonyms.compare(ipc_to_match_sentence, \\\n",
    "            #                            clc_v_1[0], seg=True, ignore=True, stopwords=False)\n",
    "        \n",
    "            \n",
    "        '-----------------------------不需要后处理了------------------------------'            \n",
    "        #上述执行完后，rank_clc_dict就存有当前ipc匹配后的得分，\n",
    "        #这时需要进行rank_clc_dict的后处理，得到最终排序前几的结果作为最终结果\n",
    "        #for rank_clc_key1, rank_clc_value1 in rank_clc_dict.items():\n",
    "            #不等于0才需要赋值\n",
    "        #    if rank_clc_value1 != 0:\n",
    "        #        for rank_clc_key2, rank_clc_value2 in rank_clc_dict.items():\n",
    "        #            if (rank_clc_key1 != rank_clc_key2) and (rank_clc_key1[0] == rank_clc_key2[0]) and \\\n",
    "        #                (rank_clc_key1 in rank_clc_key2) and (len(rank_clc_key1) <= len(rank_clc_key2)):\n",
    "        #                rank_clc_dict[rank_clc_key2] = rank_clc_dict[rank_clc_key2] + rank_clc_dict[rank_clc_key1]\n",
    "        '-----------------------------不需要后处理了------------------------------' \n",
    "        \n",
    "        \n",
    "        #对rank_clc_dict按照值大小进行排序，输出前nbest个结果\n",
    "        #rank_clc_dict_result_list，结果由大到小，包含每一条CLC分类号匹配的结果\n",
    "        rank_clc_dict_result_list = sorted(rank_clc_dict.items(), \\\n",
    "                                            key=lambda item:item[1], \\\n",
    "                                            reverse=True\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        \n",
    "        nbest_clc_key = ''\n",
    "        nbest_clc_value = ''\n",
    "        nbest_clc_frequency = ''\n",
    "        for nbest_i, value in enumerate(rank_clc_dict_result_list):\n",
    "            #只取前nbest个结果\n",
    "            if nbest_i == nbest:\n",
    "                break\n",
    "            else:   \n",
    "                nbest_clc_key = nbest_clc_key + str(value[0]) + ';'\n",
    "                nbest_clc_value0 = ''\n",
    "                for value0 in clc_dict.get(str(value[0])):\n",
    "                    nbest_clc_value0 = nbest_clc_value0 + value0\n",
    "                nbest_clc_value = nbest_clc_value + nbest_clc_value0 + ';'\n",
    "                nbest_clc_frequency = nbest_clc_frequency + str(value[1]) + ';'\n",
    "\n",
    "        clc_key_n_best_list = nbest_clc_key.rstrip(';').split(';')\n",
    "        nbest_clc_frequency_list = nbest_clc_frequency.rstrip(';').split(';')\n",
    "        #找出前几个匹配得分最高的，相同的出来\n",
    "        clc_key_n_best_list_first = []\n",
    "        clc_key_n_best_list_first.append(clc_key_n_best_list[0])\n",
    "        for clc_key_n_best_i in range(1, len(clc_key_n_best_list)):\n",
    "            if nbest_clc_frequency_list[clc_key_n_best_i] == nbest_clc_frequency_list[0]:\n",
    "                clc_key_n_best_list_first.append(clc_key_n_best_list[clc_key_n_best_i])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #匹配：只要两个列表有交集就是匹配上了\n",
    "        #for clc_key_n_best_list_first_1 in clc_key_n_best_list_first:   #--这个方案可以使准确率上升很多，但是不能用\n",
    "        if clc_key_n_best_list[0] in match_clc_key_dict[str(ipc_i)]:  #--还是这个方案最稳妥\n",
    "        #if clc_key_n_best_list_first_1 in match_clc_key_dict[str(ipc_i)]:\n",
    "            correct = correct + 1\n",
    "            ipc_section_correct_dict[str(ipc_sentence_key_list[ipc_i][0])] += 1\n",
    "            #break\n",
    "        else:\n",
    "            error_match_ipc_clc_pair.append(str(sheet['IPC分类号'][ipc_i]) + str(clc_key_n_best_list[0]))\n",
    "\n",
    "        #匹配：只要两个列表有交集就是匹配上了    \n",
    "        for each_clc_key_1 in match_clc_key_dict[str(ipc_i)]:\n",
    "            if str(each_clc_key_1) in clc_key_n_best_list:\n",
    "                top_k_correct = top_k_correct + 1\n",
    "                break\n",
    "            else:\n",
    "                topk_error_match_ipc_clc_pair.append(str(sheet['IPC分类号'][ipc_i]) + str(clc_key_n_best_list[0]))\n",
    "\n",
    "        #计算tok2-topK每一个的准确率\n",
    "        for topk_index in range(1, nbest):\n",
    "            for each_clc_key_1 in match_clc_key_dict[str(ipc_i)]:\n",
    "                if str(each_clc_key_1) in clc_key_n_best_list[:topk_index+1]:\n",
    "                    top2_to_topk_correct_dict[str(topk_index+1)] += 1\n",
    "                    break\n",
    "\n",
    "        sheet.loc[ipc_i, 'CLC分类号汇总'] = nbest_clc_key\n",
    "        sheet.loc[ipc_i, 'CLC描述'] = nbest_clc_value\n",
    "        sheet.loc[ipc_i, 'CLC匹配频次'] = nbest_clc_frequency\n",
    "        \n",
    "        #每隔 10 个ipc分类号打印一次时间\n",
    "        curr_time = datetime.datetime.now()\n",
    "        time_str = datetime.datetime.strftime(curr_time,'%Y-%m-%d %H:%M:%S')\n",
    "        if ipc_i % 50 == 0:\n",
    "            print('当前时间【{}】已经执行到第 {} 个ipc分类号了！'.format(time_str, ipc_i+1))\n",
    "            print()\n",
    "\n",
    "        \n",
    "    #将子sheet写入输出文件中\n",
    "    sheet.to_excel(writer, sheet_name=list(df)[i], index=False)\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "#所有子sheet写完后保存并关闭                 \n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "#计算准确率，TopK准确率\n",
    "ipc_count_number = len(input_ipc)\n",
    "\n",
    "acc = correct/ipc_count_number\n",
    "top_k_acc = top_k_correct/ipc_count_number\n",
    "print('\\nFinal acc: {:.2f}% .'.format(acc*100))\n",
    "print('\\nFinal top_{}_acc: {:.2f}% .'.format(nbest, top_k_acc*100))\n",
    "\n",
    "\n",
    "#结果统计\n",
    "print('\\n总数：{}\\ntop1 正确个数：{}\\ntop1 错误个数：{}\\n'.format(ipc_count_number, correct, \\\n",
    "                                                       ipc_count_number-correct))\n",
    "\n",
    "print('总数：{}\\ntopk 正确个数：{}\\ntopk 错误个数：{}\\n'.format(ipc_count_number, top_k_correct, \\\n",
    "                                                       ipc_count_number-top_k_correct))\n",
    "\n",
    "print('\\n--------------------\\n')\n",
    "for topk_name, top_correct_num in top2_to_topk_correct_dict.items():\n",
    "    print('\\nFinal top_{}_acc: {:.2f}% .'.format(topk_name, (top_correct_num/ipc_count_number)*100))\n",
    "\n",
    "print('\\n--------------------\\n')\n",
    "for topk_name, top_correct_num in top2_to_topk_correct_dict.items():\n",
    "    print('总数：{}\\ntop{} 正确个数：{}\\ntop{} 错误个数：{}\\n'.format(ipc_count_number, topk_name, top_correct_num, topk_name, ipc_count_number-top_correct_num))\n",
    "\n",
    "print('\\n--------------------\\n')\n",
    "#计算每一个IPC类别的正确个数和正确率（只有top1的）\n",
    "for sec_i, sec_cor in ipc_section_correct_dict.items():\n",
    "    print('类别：{}，总数为：{}，正确个数为：{}，错误个数为：{}，正确率为：{:.2f}%\\n'.format(sec_i, ipc_section_number_dict[sec_i], sec_cor, ipc_section_number_dict[sec_i]-sec_cor, (sec_cor/ipc_section_number_dict[sec_i])*100))\n",
    "\n",
    "print()\n",
    "print('程序执行完毕！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
